{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "2BERT-sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dunliangyang2010/Deep-Learning-practice/blob/master/BERT2_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NybIHQDVOHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fda7b10-7b71-49a6-9987-00ea924fd1de"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1JcRoZjJ-sO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14376f74-7025-42a4-de79-92fc483a68b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlExNaoZJ6Ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fccf472-87fe-4e51-a5ec-cad63c25f6a2"
      },
      "source": [
        "!pip install bert4keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert4keras in /usr/local/lib/python3.7/dist-packages (0.10.8)\n",
            "Requirement already satisfied: keras<=2.3.1 in /tensorflow-1.15.2/python3.7 (from bert4keras) (2.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.7 (from keras<=2.3.1->bert4keras) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras<=2.3.1->bert4keras) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras<=2.3.1->bert4keras) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras<=2.3.1->bert4keras) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras<=2.3.1->bert4keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras<=2.3.1->bert4keras) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras<=2.3.1->bert4keras) (3.13)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras<=2.3.1->bert4keras) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywWnm_V2J6Ei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fdf3f2-3626-4e76-b12f-95e719a601d0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
        "from bert4keras.snippets import sequence_padding\n",
        "\n",
        "from keras import layers, models, utils, losses, optimizers, callbacks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nezdSqS3J6El"
      },
      "source": [
        "# 基本参数\n",
        "maxlen = 256\n",
        "batch_size = 16\n",
        "epochs = 10000\n",
        "\n",
        "FOLDER_PATH = '/content/drive/MyDrive/class/勞動部/05-Transformer/BERT'\n",
        "BERT_MODEL_PATH = os.path.join(FOLDER_PATH, 'chinese_L-12_H-768_A-12')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrJo_T-qJ6Eo"
      },
      "source": [
        "# BERT配置\n",
        "config_path = os.path.join(BERT_MODEL_PATH, 'bert_config.json')\n",
        "checkpoint_path = os.path.join(BERT_MODEL_PATH, 'bert_model.ckpt')\n",
        "dict_path = os.path.join(BERT_MODEL_PATH, 'vocab.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qZDlQagJ6Er",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "2f6b7609-54a9-44b5-c8b6-e01e45cb0289"
      },
      "source": [
        "# Data\n",
        "df_neg = pd.read_excel(os.path.join(FOLDER_PATH, 'sentiment', 'neg_trad.xlsx'),header=None, index_col=None)\n",
        "df_pos = pd.read_excel(os.path.join(FOLDER_PATH, 'sentiment', 'pos_trad.xlsx'), header=None, index_col=None)\n",
        "df_pos['mark'] = 1\n",
        "df_neg['mark'] = 0\n",
        "df_all = pd.concat([df_pos, df_neg], ignore_index=True)\n",
        "df_all = df_all.sample(frac=1).reset_index(drop=True) # shuffle\n",
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>mark</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>外觀漂亮,各功能指示燈也很好看,價格較中關村划算一些。給女孩子用較適合,看電影、聽音樂音效還可以</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>買了此套書後，女兒非常喜歡，我本人也比較喜歡，經常陪著孩子一起閱讀，精美的圖畫和生動的語言讓...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>這本書沒有我想象的好，書本身的質量和印刷還可以 ，但內容與書名不符，應該改名字；而且有很多牽...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>一直以來對日本這個民族沒有好感，但是，對他的文化始終有濃厚的興趣，各方面的信息拉拉雜雜也了解...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我真的建議所有到成都出差的朋友千萬不要入住這家酒店，首先這個酒店的位置實在不好，附近基本上沒...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  mark\n",
              "0   外觀漂亮,各功能指示燈也很好看,價格較中關村划算一些。給女孩子用較適合,看電影、聽音樂音效還可以     1\n",
              "1  買了此套書後，女兒非常喜歡，我本人也比較喜歡，經常陪著孩子一起閱讀，精美的圖畫和生動的語言讓...     0\n",
              "2  這本書沒有我想象的好，書本身的質量和印刷還可以 ，但內容與書名不符，應該改名字；而且有很多牽...     0\n",
              "3  一直以來對日本這個民族沒有好感，但是，對他的文化始終有濃厚的興趣，各方面的信息拉拉雜雜也了解...     1\n",
              "4  我真的建議所有到成都出差的朋友千萬不要入住這家酒店，首先這個酒店的位置實在不好，附近基本上沒...     0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVw6oN4tJ6Ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2029e35-2f21-4664-cb41-b02acd6ab358"
      },
      "source": [
        "# split data\n",
        "val_ratio = 0.2\n",
        "df_train = df_all.iloc[:-int(len(df_all) * 0.2), :]\n",
        "df_val = df_all.iloc[-int(len(df_all) * 0.2):, :]\n",
        "df_train.shape, df_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16884, 2), (4221, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTvaNmr0J6Ex"
      },
      "source": [
        "# 載入精簡詞表，建立分詞器\n",
        "token_dict, keep_tokens = load_vocab(\n",
        "    dict_path=dict_path,\n",
        "    simplified=True,\n",
        "    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n",
        ")\n",
        "tokenizer = Tokenizer(token_dict, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlWTyb3rC1Tj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4f2ff1-1941-4cd1-c885-eb7d3a808384"
      },
      "source": [
        "token_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[PAD]': 0,\n",
              " '[UNK]': 1,\n",
              " '[CLS]': 2,\n",
              " '[SEP]': 3,\n",
              " '!': 4,\n",
              " '\"': 5,\n",
              " '#': 6,\n",
              " '$': 7,\n",
              " '%': 8,\n",
              " '&': 9,\n",
              " \"'\": 10,\n",
              " '(': 11,\n",
              " ')': 12,\n",
              " '*': 13,\n",
              " '+': 14,\n",
              " ',': 15,\n",
              " '-': 16,\n",
              " '.': 17,\n",
              " '/': 18,\n",
              " '0': 19,\n",
              " '1': 20,\n",
              " '2': 21,\n",
              " '3': 22,\n",
              " '4': 23,\n",
              " '5': 24,\n",
              " '6': 25,\n",
              " '7': 26,\n",
              " '8': 27,\n",
              " '9': 28,\n",
              " ':': 29,\n",
              " ';': 30,\n",
              " '<': 31,\n",
              " '=': 32,\n",
              " '>': 33,\n",
              " '?': 34,\n",
              " '@': 35,\n",
              " '[': 36,\n",
              " '\\\\': 37,\n",
              " ']': 38,\n",
              " '^': 39,\n",
              " '_': 40,\n",
              " 'a': 41,\n",
              " 'b': 42,\n",
              " 'c': 43,\n",
              " 'd': 44,\n",
              " 'e': 45,\n",
              " 'f': 46,\n",
              " 'g': 47,\n",
              " 'h': 48,\n",
              " 'i': 49,\n",
              " 'j': 50,\n",
              " 'k': 51,\n",
              " 'l': 52,\n",
              " 'm': 53,\n",
              " 'n': 54,\n",
              " 'o': 55,\n",
              " 'p': 56,\n",
              " 'q': 57,\n",
              " 'r': 58,\n",
              " 's': 59,\n",
              " 't': 60,\n",
              " 'u': 61,\n",
              " 'v': 62,\n",
              " 'w': 63,\n",
              " 'x': 64,\n",
              " 'y': 65,\n",
              " 'z': 66,\n",
              " '{': 67,\n",
              " '|': 68,\n",
              " '}': 69,\n",
              " '~': 70,\n",
              " '£': 71,\n",
              " '¤': 72,\n",
              " '¥': 73,\n",
              " '§': 74,\n",
              " '©': 75,\n",
              " '«': 76,\n",
              " '®': 77,\n",
              " '°': 78,\n",
              " '±': 79,\n",
              " '²': 80,\n",
              " '³': 81,\n",
              " 'µ': 82,\n",
              " '·': 83,\n",
              " '¹': 84,\n",
              " 'º': 85,\n",
              " '»': 86,\n",
              " '¼': 87,\n",
              " '×': 88,\n",
              " 'ß': 89,\n",
              " 'æ': 90,\n",
              " '÷': 91,\n",
              " 'ø': 92,\n",
              " 'đ': 93,\n",
              " 'ŋ': 94,\n",
              " 'ɔ': 95,\n",
              " 'ə': 96,\n",
              " 'ɡ': 97,\n",
              " 'ʰ': 98,\n",
              " 'ˇ': 99,\n",
              " 'ˈ': 100,\n",
              " 'ˊ': 101,\n",
              " 'ˋ': 102,\n",
              " 'ˍ': 103,\n",
              " 'ː': 104,\n",
              " '˙': 105,\n",
              " '˚': 106,\n",
              " 'ˢ': 107,\n",
              " 'α': 108,\n",
              " 'β': 109,\n",
              " 'γ': 110,\n",
              " 'δ': 111,\n",
              " 'ε': 112,\n",
              " 'η': 113,\n",
              " 'θ': 114,\n",
              " 'ι': 115,\n",
              " 'κ': 116,\n",
              " 'λ': 117,\n",
              " 'μ': 118,\n",
              " 'ν': 119,\n",
              " 'ο': 120,\n",
              " 'π': 121,\n",
              " 'ρ': 122,\n",
              " 'ς': 123,\n",
              " 'σ': 124,\n",
              " 'τ': 125,\n",
              " 'υ': 126,\n",
              " 'φ': 127,\n",
              " 'χ': 128,\n",
              " 'ψ': 129,\n",
              " 'ω': 130,\n",
              " 'а': 131,\n",
              " 'б': 132,\n",
              " 'в': 133,\n",
              " 'г': 134,\n",
              " 'д': 135,\n",
              " 'е': 136,\n",
              " 'ж': 137,\n",
              " 'з': 138,\n",
              " 'и': 139,\n",
              " 'к': 140,\n",
              " 'л': 141,\n",
              " 'м': 142,\n",
              " 'н': 143,\n",
              " 'о': 144,\n",
              " 'п': 145,\n",
              " 'р': 146,\n",
              " 'с': 147,\n",
              " 'т': 148,\n",
              " 'у': 149,\n",
              " 'ф': 150,\n",
              " 'х': 151,\n",
              " 'ц': 152,\n",
              " 'ч': 153,\n",
              " 'ш': 154,\n",
              " 'ы': 155,\n",
              " 'ь': 156,\n",
              " 'я': 157,\n",
              " 'і': 158,\n",
              " 'ا': 159,\n",
              " 'ب': 160,\n",
              " 'ة': 161,\n",
              " 'ت': 162,\n",
              " 'د': 163,\n",
              " 'ر': 164,\n",
              " 'س': 165,\n",
              " 'ع': 166,\n",
              " 'ل': 167,\n",
              " 'م': 168,\n",
              " 'ن': 169,\n",
              " 'ه': 170,\n",
              " 'و': 171,\n",
              " 'ي': 172,\n",
              " '۩': 173,\n",
              " 'ก': 174,\n",
              " 'ง': 175,\n",
              " 'น': 176,\n",
              " 'ม': 177,\n",
              " 'ย': 178,\n",
              " 'ร': 179,\n",
              " 'อ': 180,\n",
              " 'า': 181,\n",
              " 'เ': 182,\n",
              " '๑': 183,\n",
              " '་': 184,\n",
              " 'ღ': 185,\n",
              " 'ᄀ': 186,\n",
              " 'ᄁ': 187,\n",
              " 'ᄂ': 188,\n",
              " 'ᄃ': 189,\n",
              " 'ᄅ': 190,\n",
              " 'ᄆ': 191,\n",
              " 'ᄇ': 192,\n",
              " 'ᄈ': 193,\n",
              " 'ᄉ': 194,\n",
              " 'ᄋ': 195,\n",
              " 'ᄌ': 196,\n",
              " 'ᄎ': 197,\n",
              " 'ᄏ': 198,\n",
              " 'ᄐ': 199,\n",
              " 'ᄑ': 200,\n",
              " 'ᄒ': 201,\n",
              " 'ᅡ': 202,\n",
              " 'ᅢ': 203,\n",
              " 'ᅣ': 204,\n",
              " 'ᅥ': 205,\n",
              " 'ᅦ': 206,\n",
              " 'ᅧ': 207,\n",
              " 'ᅨ': 208,\n",
              " 'ᅩ': 209,\n",
              " 'ᅪ': 210,\n",
              " 'ᅬ': 211,\n",
              " 'ᅭ': 212,\n",
              " 'ᅮ': 213,\n",
              " 'ᅯ': 214,\n",
              " 'ᅲ': 215,\n",
              " 'ᅳ': 216,\n",
              " 'ᅴ': 217,\n",
              " 'ᅵ': 218,\n",
              " 'ᆨ': 219,\n",
              " 'ᆫ': 220,\n",
              " 'ᆯ': 221,\n",
              " 'ᆷ': 222,\n",
              " 'ᆸ': 223,\n",
              " 'ᆺ': 224,\n",
              " 'ᆻ': 225,\n",
              " 'ᆼ': 226,\n",
              " 'ᗜ': 227,\n",
              " 'ᵃ': 228,\n",
              " 'ᵉ': 229,\n",
              " 'ᵍ': 230,\n",
              " 'ᵏ': 231,\n",
              " 'ᵐ': 232,\n",
              " 'ᵒ': 233,\n",
              " 'ᵘ': 234,\n",
              " '‖': 235,\n",
              " '„': 236,\n",
              " '†': 237,\n",
              " '•': 238,\n",
              " '‥': 239,\n",
              " '‧': 240,\n",
              " '': 241,\n",
              " '‰': 242,\n",
              " '′': 243,\n",
              " '″': 244,\n",
              " '‹': 245,\n",
              " '›': 246,\n",
              " '※': 247,\n",
              " '‿': 248,\n",
              " '⁄': 249,\n",
              " 'ⁱ': 250,\n",
              " '⁺': 251,\n",
              " 'ⁿ': 252,\n",
              " '₁': 253,\n",
              " '₂': 254,\n",
              " '₃': 255,\n",
              " '₄': 256,\n",
              " '€': 257,\n",
              " '℃': 258,\n",
              " '№': 259,\n",
              " '™': 260,\n",
              " 'ⅰ': 261,\n",
              " 'ⅱ': 262,\n",
              " 'ⅲ': 263,\n",
              " 'ⅳ': 264,\n",
              " 'ⅴ': 265,\n",
              " '←': 266,\n",
              " '↑': 267,\n",
              " '→': 268,\n",
              " '↓': 269,\n",
              " '↔': 270,\n",
              " '↗': 271,\n",
              " '↘': 272,\n",
              " '⇒': 273,\n",
              " '∀': 274,\n",
              " '−': 275,\n",
              " '∕': 276,\n",
              " '∙': 277,\n",
              " '√': 278,\n",
              " '∞': 279,\n",
              " '∟': 280,\n",
              " '∠': 281,\n",
              " '∣': 282,\n",
              " '∥': 283,\n",
              " '∩': 284,\n",
              " '∮': 285,\n",
              " '∶': 286,\n",
              " '∼': 287,\n",
              " '∽': 288,\n",
              " '≈': 289,\n",
              " '≒': 290,\n",
              " '≡': 291,\n",
              " '≤': 292,\n",
              " '≥': 293,\n",
              " '≦': 294,\n",
              " '≧': 295,\n",
              " '≪': 296,\n",
              " '≫': 297,\n",
              " '⊙': 298,\n",
              " '⋅': 299,\n",
              " '⋈': 300,\n",
              " '⋯': 301,\n",
              " '⌒': 302,\n",
              " '①': 303,\n",
              " '②': 304,\n",
              " '③': 305,\n",
              " '④': 306,\n",
              " '⑤': 307,\n",
              " '⑥': 308,\n",
              " '⑦': 309,\n",
              " '⑧': 310,\n",
              " '⑨': 311,\n",
              " '⑩': 312,\n",
              " '⑴': 313,\n",
              " '⑵': 314,\n",
              " '⑶': 315,\n",
              " '⑷': 316,\n",
              " '⑸': 317,\n",
              " '⒈': 318,\n",
              " '⒉': 319,\n",
              " '⒊': 320,\n",
              " '⒋': 321,\n",
              " 'ⓒ': 322,\n",
              " 'ⓔ': 323,\n",
              " 'ⓘ': 324,\n",
              " '─': 325,\n",
              " '━': 326,\n",
              " '│': 327,\n",
              " '┃': 328,\n",
              " '┅': 329,\n",
              " '┆': 330,\n",
              " '┊': 331,\n",
              " '┌': 332,\n",
              " '└': 333,\n",
              " '├': 334,\n",
              " '┣': 335,\n",
              " '═': 336,\n",
              " '║': 337,\n",
              " '╚': 338,\n",
              " '╞': 339,\n",
              " '╠': 340,\n",
              " '╭': 341,\n",
              " '╮': 342,\n",
              " '╯': 343,\n",
              " '╰': 344,\n",
              " '╱': 345,\n",
              " '╳': 346,\n",
              " '▂': 347,\n",
              " '▃': 348,\n",
              " '▅': 349,\n",
              " '▇': 350,\n",
              " '█': 351,\n",
              " '▉': 352,\n",
              " '▋': 353,\n",
              " '▌': 354,\n",
              " '▍': 355,\n",
              " '▎': 356,\n",
              " '■': 357,\n",
              " '□': 358,\n",
              " '▪': 359,\n",
              " '▫': 360,\n",
              " '▬': 361,\n",
              " '▲': 362,\n",
              " '△': 363,\n",
              " '▶': 364,\n",
              " '►': 365,\n",
              " '▼': 366,\n",
              " '▽': 367,\n",
              " '◆': 368,\n",
              " '◇': 369,\n",
              " '○': 370,\n",
              " '◎': 371,\n",
              " '●': 372,\n",
              " '◕': 373,\n",
              " '◠': 374,\n",
              " '◢': 375,\n",
              " '◤': 376,\n",
              " '☀': 377,\n",
              " '★': 378,\n",
              " '☆': 379,\n",
              " '☕': 380,\n",
              " '☞': 381,\n",
              " '☺': 382,\n",
              " '☼': 383,\n",
              " '♀': 384,\n",
              " '♂': 385,\n",
              " '♠': 386,\n",
              " '♡': 387,\n",
              " '♣': 388,\n",
              " '♥': 389,\n",
              " '♦': 390,\n",
              " '♪': 391,\n",
              " '♫': 392,\n",
              " '♬': 393,\n",
              " '✈': 394,\n",
              " '✔': 395,\n",
              " '✕': 396,\n",
              " '✖': 397,\n",
              " '✦': 398,\n",
              " '✨': 399,\n",
              " '✪': 400,\n",
              " '✰': 401,\n",
              " '✿': 402,\n",
              " '❀': 403,\n",
              " '❤': 404,\n",
              " '➜': 405,\n",
              " '➤': 406,\n",
              " '⦿': 407,\n",
              " '、': 408,\n",
              " '。': 409,\n",
              " '〃': 410,\n",
              " '々': 411,\n",
              " '〇': 412,\n",
              " '〈': 413,\n",
              " '〉': 414,\n",
              " '《': 415,\n",
              " '》': 416,\n",
              " '「': 417,\n",
              " '」': 418,\n",
              " '『': 419,\n",
              " '』': 420,\n",
              " '【': 421,\n",
              " '】': 422,\n",
              " '〓': 423,\n",
              " '〔': 424,\n",
              " '〕': 425,\n",
              " '〖': 426,\n",
              " '〗': 427,\n",
              " '〜': 428,\n",
              " '〝': 429,\n",
              " '〞': 430,\n",
              " 'ぁ': 431,\n",
              " 'あ': 432,\n",
              " 'ぃ': 433,\n",
              " 'い': 434,\n",
              " 'う': 435,\n",
              " 'ぇ': 436,\n",
              " 'え': 437,\n",
              " 'お': 438,\n",
              " 'か': 439,\n",
              " 'き': 440,\n",
              " 'く': 441,\n",
              " 'け': 442,\n",
              " 'こ': 443,\n",
              " 'さ': 444,\n",
              " 'し': 445,\n",
              " 'す': 446,\n",
              " 'せ': 447,\n",
              " 'そ': 448,\n",
              " 'た': 449,\n",
              " 'ち': 450,\n",
              " 'っ': 451,\n",
              " 'つ': 452,\n",
              " 'て': 453,\n",
              " 'と': 454,\n",
              " 'な': 455,\n",
              " 'に': 456,\n",
              " 'ぬ': 457,\n",
              " 'ね': 458,\n",
              " 'の': 459,\n",
              " 'は': 460,\n",
              " 'ひ': 461,\n",
              " 'ふ': 462,\n",
              " 'へ': 463,\n",
              " 'ほ': 464,\n",
              " 'ま': 465,\n",
              " 'み': 466,\n",
              " 'む': 467,\n",
              " 'め': 468,\n",
              " 'も': 469,\n",
              " 'ゃ': 470,\n",
              " 'や': 471,\n",
              " 'ゅ': 472,\n",
              " 'ゆ': 473,\n",
              " 'ょ': 474,\n",
              " 'よ': 475,\n",
              " 'ら': 476,\n",
              " 'り': 477,\n",
              " 'る': 478,\n",
              " 'れ': 479,\n",
              " 'ろ': 480,\n",
              " 'わ': 481,\n",
              " 'を': 482,\n",
              " 'ん': 483,\n",
              " '゜': 484,\n",
              " 'ゝ': 485,\n",
              " 'ァ': 486,\n",
              " 'ア': 487,\n",
              " 'ィ': 488,\n",
              " 'イ': 489,\n",
              " 'ゥ': 490,\n",
              " 'ウ': 491,\n",
              " 'ェ': 492,\n",
              " 'エ': 493,\n",
              " 'ォ': 494,\n",
              " 'オ': 495,\n",
              " 'カ': 496,\n",
              " 'キ': 497,\n",
              " 'ク': 498,\n",
              " 'ケ': 499,\n",
              " 'コ': 500,\n",
              " 'サ': 501,\n",
              " 'シ': 502,\n",
              " 'ス': 503,\n",
              " 'セ': 504,\n",
              " 'ソ': 505,\n",
              " 'タ': 506,\n",
              " 'チ': 507,\n",
              " 'ッ': 508,\n",
              " 'ツ': 509,\n",
              " 'テ': 510,\n",
              " 'ト': 511,\n",
              " 'ナ': 512,\n",
              " 'ニ': 513,\n",
              " 'ヌ': 514,\n",
              " 'ネ': 515,\n",
              " 'ノ': 516,\n",
              " 'ハ': 517,\n",
              " 'ヒ': 518,\n",
              " 'フ': 519,\n",
              " 'ヘ': 520,\n",
              " 'ホ': 521,\n",
              " 'マ': 522,\n",
              " 'ミ': 523,\n",
              " 'ム': 524,\n",
              " 'メ': 525,\n",
              " 'モ': 526,\n",
              " 'ャ': 527,\n",
              " 'ヤ': 528,\n",
              " 'ュ': 529,\n",
              " 'ユ': 530,\n",
              " 'ョ': 531,\n",
              " 'ヨ': 532,\n",
              " 'ラ': 533,\n",
              " 'リ': 534,\n",
              " 'ル': 535,\n",
              " 'レ': 536,\n",
              " 'ロ': 537,\n",
              " 'ワ': 538,\n",
              " 'ヲ': 539,\n",
              " 'ン': 540,\n",
              " 'ヶ': 541,\n",
              " '・': 542,\n",
              " 'ー': 543,\n",
              " 'ヽ': 544,\n",
              " 'ㄅ': 545,\n",
              " 'ㄆ': 546,\n",
              " 'ㄇ': 547,\n",
              " 'ㄉ': 548,\n",
              " 'ㄋ': 549,\n",
              " 'ㄌ': 550,\n",
              " 'ㄍ': 551,\n",
              " 'ㄎ': 552,\n",
              " 'ㄏ': 553,\n",
              " 'ㄒ': 554,\n",
              " 'ㄚ': 555,\n",
              " 'ㄛ': 556,\n",
              " 'ㄞ': 557,\n",
              " 'ㄟ': 558,\n",
              " 'ㄢ': 559,\n",
              " 'ㄤ': 560,\n",
              " 'ㄥ': 561,\n",
              " 'ㄧ': 562,\n",
              " 'ㄨ': 563,\n",
              " 'ㆍ': 564,\n",
              " '㈦': 565,\n",
              " '㊣': 566,\n",
              " '㎡': 567,\n",
              " '㗎': 568,\n",
              " '一': 569,\n",
              " '丁': 570,\n",
              " '七': 571,\n",
              " '万': 572,\n",
              " '丈': 573,\n",
              " '三': 574,\n",
              " '上': 575,\n",
              " '下': 576,\n",
              " '不': 577,\n",
              " '与': 578,\n",
              " '丐': 579,\n",
              " '丑': 580,\n",
              " '专': 581,\n",
              " '且': 582,\n",
              " '丕': 583,\n",
              " '世': 584,\n",
              " '丘': 585,\n",
              " '丙': 586,\n",
              " '业': 587,\n",
              " '丛': 588,\n",
              " '东': 589,\n",
              " '丝': 590,\n",
              " '丞': 591,\n",
              " '丟': 592,\n",
              " '両': 593,\n",
              " '丢': 594,\n",
              " '两': 595,\n",
              " '严': 596,\n",
              " '並': 597,\n",
              " '丧': 598,\n",
              " '丨': 599,\n",
              " '个': 600,\n",
              " '丫': 601,\n",
              " '中': 602,\n",
              " '丰': 603,\n",
              " '串': 604,\n",
              " '临': 605,\n",
              " '丶': 606,\n",
              " '丸': 607,\n",
              " '丹': 608,\n",
              " '为': 609,\n",
              " '主': 610,\n",
              " '丼': 611,\n",
              " '丽': 612,\n",
              " '举': 613,\n",
              " '丿': 614,\n",
              " '乂': 615,\n",
              " '乃': 616,\n",
              " '久': 617,\n",
              " '么': 618,\n",
              " '义': 619,\n",
              " '之': 620,\n",
              " '乌': 621,\n",
              " '乍': 622,\n",
              " '乎': 623,\n",
              " '乏': 624,\n",
              " '乐': 625,\n",
              " '乒': 626,\n",
              " '乓': 627,\n",
              " '乔': 628,\n",
              " '乖': 629,\n",
              " '乗': 630,\n",
              " '乘': 631,\n",
              " '乙': 632,\n",
              " '乜': 633,\n",
              " '九': 634,\n",
              " '乞': 635,\n",
              " '也': 636,\n",
              " '习': 637,\n",
              " '乡': 638,\n",
              " '书': 639,\n",
              " '乩': 640,\n",
              " '买': 641,\n",
              " '乱': 642,\n",
              " '乳': 643,\n",
              " '乾': 644,\n",
              " '亀': 645,\n",
              " '亂': 646,\n",
              " '了': 647,\n",
              " '予': 648,\n",
              " '争': 649,\n",
              " '事': 650,\n",
              " '二': 651,\n",
              " '于': 652,\n",
              " '亏': 653,\n",
              " '云': 654,\n",
              " '互': 655,\n",
              " '五': 656,\n",
              " '井': 657,\n",
              " '亘': 658,\n",
              " '亙': 659,\n",
              " '亚': 660,\n",
              " '些': 661,\n",
              " '亜': 662,\n",
              " '亞': 663,\n",
              " '亟': 664,\n",
              " '亡': 665,\n",
              " '亢': 666,\n",
              " '交': 667,\n",
              " '亥': 668,\n",
              " '亦': 669,\n",
              " '产': 670,\n",
              " '亨': 671,\n",
              " '亩': 672,\n",
              " '享': 673,\n",
              " '京': 674,\n",
              " '亭': 675,\n",
              " '亮': 676,\n",
              " '亲': 677,\n",
              " '亳': 678,\n",
              " '亵': 679,\n",
              " '人': 680,\n",
              " '亿': 681,\n",
              " '什': 682,\n",
              " '仁': 683,\n",
              " '仃': 684,\n",
              " '仄': 685,\n",
              " '仅': 686,\n",
              " '仆': 687,\n",
              " '仇': 688,\n",
              " '今': 689,\n",
              " '介': 690,\n",
              " '仍': 691,\n",
              " '从': 692,\n",
              " '仏': 693,\n",
              " '仑': 694,\n",
              " '仓': 695,\n",
              " '仔': 696,\n",
              " '仕': 697,\n",
              " '他': 698,\n",
              " '仗': 699,\n",
              " '付': 700,\n",
              " '仙': 701,\n",
              " '仝': 702,\n",
              " '仞': 703,\n",
              " '仟': 704,\n",
              " '代': 705,\n",
              " '令': 706,\n",
              " '以': 707,\n",
              " '仨': 708,\n",
              " '仪': 709,\n",
              " '们': 710,\n",
              " '仮': 711,\n",
              " '仰': 712,\n",
              " '仲': 713,\n",
              " '件': 714,\n",
              " '价': 715,\n",
              " '任': 716,\n",
              " '份': 717,\n",
              " '仿': 718,\n",
              " '企': 719,\n",
              " '伉': 720,\n",
              " '伊': 721,\n",
              " '伍': 722,\n",
              " '伎': 723,\n",
              " '伏': 724,\n",
              " '伐': 725,\n",
              " '休': 726,\n",
              " '伕': 727,\n",
              " '众': 728,\n",
              " '优': 729,\n",
              " '伙': 730,\n",
              " '会': 731,\n",
              " '伝': 732,\n",
              " '伞': 733,\n",
              " '伟': 734,\n",
              " '传': 735,\n",
              " '伢': 736,\n",
              " '伤': 737,\n",
              " '伦': 738,\n",
              " '伪': 739,\n",
              " '伫': 740,\n",
              " '伯': 741,\n",
              " '估': 742,\n",
              " '伴': 743,\n",
              " '伶': 744,\n",
              " '伸': 745,\n",
              " '伺': 746,\n",
              " '似': 747,\n",
              " '伽': 748,\n",
              " '佃': 749,\n",
              " '但': 750,\n",
              " '佇': 751,\n",
              " '佈': 752,\n",
              " '位': 753,\n",
              " '低': 754,\n",
              " '住': 755,\n",
              " '佐': 756,\n",
              " '佑': 757,\n",
              " '体': 758,\n",
              " '佔': 759,\n",
              " '何': 760,\n",
              " '佗': 761,\n",
              " '佘': 762,\n",
              " '余': 763,\n",
              " '佚': 764,\n",
              " '佛': 765,\n",
              " '作': 766,\n",
              " '佝': 767,\n",
              " '佞': 768,\n",
              " '佟': 769,\n",
              " '你': 770,\n",
              " '佢': 771,\n",
              " '佣': 772,\n",
              " '佤': 773,\n",
              " '佥': 774,\n",
              " '佩': 775,\n",
              " '佬': 776,\n",
              " '佯': 777,\n",
              " '佰': 778,\n",
              " '佳': 779,\n",
              " '併': 780,\n",
              " '佶': 781,\n",
              " '佻': 782,\n",
              " '佼': 783,\n",
              " '使': 784,\n",
              " '侃': 785,\n",
              " '侄': 786,\n",
              " '來': 787,\n",
              " '侈': 788,\n",
              " '例': 789,\n",
              " '侍': 790,\n",
              " '侏': 791,\n",
              " '侑': 792,\n",
              " '侖': 793,\n",
              " '侗': 794,\n",
              " '供': 795,\n",
              " '依': 796,\n",
              " '侠': 797,\n",
              " '価': 798,\n",
              " '侣': 799,\n",
              " '侥': 800,\n",
              " '侦': 801,\n",
              " '侧': 802,\n",
              " '侨': 803,\n",
              " '侬': 804,\n",
              " '侮': 805,\n",
              " '侯': 806,\n",
              " '侵': 807,\n",
              " '侶': 808,\n",
              " '侷': 809,\n",
              " '便': 810,\n",
              " '係': 811,\n",
              " '促': 812,\n",
              " '俄': 813,\n",
              " '俊': 814,\n",
              " '俎': 815,\n",
              " '俏': 816,\n",
              " '俐': 817,\n",
              " '俑': 818,\n",
              " '俗': 819,\n",
              " '俘': 820,\n",
              " '俚': 821,\n",
              " '保': 822,\n",
              " '俞': 823,\n",
              " '俟': 824,\n",
              " '俠': 825,\n",
              " '信': 826,\n",
              " '俨': 827,\n",
              " '俩': 828,\n",
              " '俪': 829,\n",
              " '俬': 830,\n",
              " '俭': 831,\n",
              " '修': 832,\n",
              " '俯': 833,\n",
              " '俱': 834,\n",
              " '俳': 835,\n",
              " '俸': 836,\n",
              " '俺': 837,\n",
              " '俾': 838,\n",
              " '倆': 839,\n",
              " '倉': 840,\n",
              " '個': 841,\n",
              " '倌': 842,\n",
              " '倍': 843,\n",
              " '倏': 844,\n",
              " '們': 845,\n",
              " '倒': 846,\n",
              " '倔': 847,\n",
              " '倖': 848,\n",
              " '倘': 849,\n",
              " '候': 850,\n",
              " '倚': 851,\n",
              " '倜': 852,\n",
              " '借': 853,\n",
              " '倡': 854,\n",
              " '値': 855,\n",
              " '倦': 856,\n",
              " '倩': 857,\n",
              " '倪': 858,\n",
              " '倫': 859,\n",
              " '倬': 860,\n",
              " '倭': 861,\n",
              " '倶': 862,\n",
              " '债': 863,\n",
              " '值': 864,\n",
              " '倾': 865,\n",
              " '偃': 866,\n",
              " '假': 867,\n",
              " '偈': 868,\n",
              " '偉': 869,\n",
              " '偌': 870,\n",
              " '偎': 871,\n",
              " '偏': 872,\n",
              " '偕': 873,\n",
              " '做': 874,\n",
              " '停': 875,\n",
              " '健': 876,\n",
              " '側': 877,\n",
              " '偵': 878,\n",
              " '偶': 879,\n",
              " '偷': 880,\n",
              " '偻': 881,\n",
              " '偽': 882,\n",
              " '偿': 883,\n",
              " '傀': 884,\n",
              " '傅': 885,\n",
              " '傍': 886,\n",
              " '傑': 887,\n",
              " '傘': 888,\n",
              " '備': 889,\n",
              " '傚': 890,\n",
              " '傢': 891,\n",
              " '傣': 892,\n",
              " '傥': 893,\n",
              " '储': 894,\n",
              " '傩': 895,\n",
              " '催': 896,\n",
              " '傭': 897,\n",
              " '傲': 898,\n",
              " '傳': 899,\n",
              " '債': 900,\n",
              " '傷': 901,\n",
              " '傻': 902,\n",
              " '傾': 903,\n",
              " '僅': 904,\n",
              " '働': 905,\n",
              " '像': 906,\n",
              " '僑': 907,\n",
              " '僕': 908,\n",
              " '僖': 909,\n",
              " '僚': 910,\n",
              " '僥': 911,\n",
              " '僧': 912,\n",
              " '僭': 913,\n",
              " '僮': 914,\n",
              " '僱': 915,\n",
              " '僵': 916,\n",
              " '價': 917,\n",
              " '僻': 918,\n",
              " '儀': 919,\n",
              " '儂': 920,\n",
              " '億': 921,\n",
              " '儆': 922,\n",
              " '儉': 923,\n",
              " '儋': 924,\n",
              " '儒': 925,\n",
              " '儕': 926,\n",
              " '儘': 927,\n",
              " '償': 928,\n",
              " '儡': 929,\n",
              " '優': 930,\n",
              " '儲': 931,\n",
              " '儷': 932,\n",
              " '儼': 933,\n",
              " '儿': 934,\n",
              " '兀': 935,\n",
              " '允': 936,\n",
              " '元': 937,\n",
              " '兄': 938,\n",
              " '充': 939,\n",
              " '兆': 940,\n",
              " '兇': 941,\n",
              " '先': 942,\n",
              " '光': 943,\n",
              " '克': 944,\n",
              " '兌': 945,\n",
              " '免': 946,\n",
              " '児': 947,\n",
              " '兑': 948,\n",
              " '兒': 949,\n",
              " '兔': 950,\n",
              " '兖': 951,\n",
              " '党': 952,\n",
              " '兜': 953,\n",
              " '兢': 954,\n",
              " '入': 955,\n",
              " '內': 956,\n",
              " '全': 957,\n",
              " '兩': 958,\n",
              " '八': 959,\n",
              " '公': 960,\n",
              " '六': 961,\n",
              " '兮': 962,\n",
              " '兰': 963,\n",
              " '共': 964,\n",
              " '兲': 965,\n",
              " '关': 966,\n",
              " '兴': 967,\n",
              " '兵': 968,\n",
              " '其': 969,\n",
              " '具': 970,\n",
              " '典': 971,\n",
              " '兹': 972,\n",
              " '养': 973,\n",
              " '兼': 974,\n",
              " '兽': 975,\n",
              " '冀': 976,\n",
              " '内': 977,\n",
              " '円': 978,\n",
              " '冇': 979,\n",
              " '冈': 980,\n",
              " '冉': 981,\n",
              " '冊': 982,\n",
              " '册': 983,\n",
              " '再': 984,\n",
              " '冏': 985,\n",
              " '冒': 986,\n",
              " '冕': 987,\n",
              " '冗': 988,\n",
              " '写': 989,\n",
              " '军': 990,\n",
              " '农': 991,\n",
              " '冠': 992,\n",
              " '冢': 993,\n",
              " '冤': 994,\n",
              " '冥': 995,\n",
              " '冨': 996,\n",
              " '冪': 997,\n",
              " '冬': 998,\n",
              " '冯': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AwdYWBcJ6Ez"
      },
      "source": [
        "def data_generator(df, batch_size):\n",
        "    '''data generator for fit_generator'''\n",
        "    n = len(df)\n",
        "    i = 0\n",
        "    df = df\n",
        "    while True:\n",
        "        batch_token_ids, batch_segment_ids, labels = [], [], []\n",
        "        for b in range(batch_size):\n",
        "            if i==0:\n",
        "                # shuffle\n",
        "                df = df.sample(frac=1).reset_index(drop=True)\n",
        "            comment = df.iloc[i, 0]\n",
        "            label = df.iloc[i, 1]\n",
        "            token_ids, segment_ids = tokenizer.encode(\n",
        "                comment, maxlen=maxlen\n",
        "            )\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            labels.append(label)\n",
        "            i = (i+1) % n\n",
        "            \n",
        "        batch_token_ids = sequence_padding(batch_token_ids)\n",
        "        batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "        yield [batch_token_ids, batch_segment_ids], np.expand_dims(np.array(labels), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SA7kcsMJ6E2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ac3117-e5e9-487b-87f4-2c9eaebc6ae2"
      },
      "source": [
        "train_generator = data_generator(df_all, batch_size=1)\n",
        "next(train_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([[   2, 4056,  682, 7836, 6741, 6413, 6755, 7836, 2612, 2667, 8023,\n",
              "          5896,  576, 4536, 1504, 1992,   15, 2480, 7836, 6815, 3658, 1066,\n",
              "            34, 2667, 2491, 5763, 4398, 6755,  661, 3190,   15, 2259, 3205,\n",
              "          5441, 1660, 6755,  841, 6765, 3212, 1082, 6741, 1066,   17, 6240,\n",
              "          6240,   17,    3]]),\n",
              "  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0]])],\n",
              " array([[0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v2vymN4J6E4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e190c4f8-18c5-43fa-8c9e-e427112641cc"
      },
      "source": [
        "bert_model = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,  # 只保留keep_tokens中的字，精簡原字表\n",
        ")\n",
        "\n",
        "# # Freeze BERT layers\n",
        "# for l in bert_model.layers:\n",
        "#     l.trainable = False\n",
        "\n",
        "x = layers.Lambda(lambda x: x[:, 0])(bert_model.output)\n",
        "pred = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = models.Model(bert_model.input, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DO1uGEgJ6E6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd01344-9939-4943-a78b-c7cc9a5c8dd4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     multiple             10432512    Input-Token[0][0]                \n",
            "                                                                 MLM-Norm[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Attention-UniLM-Mask (Lambda)   (None, 1, None, None 0           Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
            "                                                                 Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "                                                                 Transformer-0-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "                                                                 Transformer-1-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "                                                                 Transformer-2-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "                                                                 Transformer-3-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "                                                                 Transformer-4-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "                                                                 Transformer-5-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "                                                                 Transformer-6-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "                                                                 Transformer-7-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "                                                                 Transformer-8-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "                                                                 Transformer-9-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "                                                                 Transformer-10-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "                                                                 Transformer-11-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-11-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Bias (BiasAdd)              (None, None, 13584)  13584       Embedding-Token[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Activation (Activation)     (None, None, 13584)  0           MLM-Bias[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 13584)        0           MLM-Activation[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 1)            13585       lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 96,502,561\n",
            "Trainable params: 96,502,561\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY9Ee059J6E8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4719e17-fa16-4971-eae9-9f96dc77b13d"
      },
      "source": [
        "model.compile(\n",
        "    loss=losses.binary_crossentropy,\n",
        "    optimizer=optimizers.Adam(1e-5), # smaller learning rate\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHu8KuiHJ6FA"
      },
      "source": [
        "data_gen_train = data_generator(df_train, batch_size)\n",
        "data_gen_val = data_generator(df_val, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYIfjugRJ6FC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "99d1012c-e68c-46a8-9ea2-a683e27c7795"
      },
      "source": [
        "model.fit_generator(\n",
        "    data_gen_train,\n",
        "    steps_per_epoch=len(df_train) // batch_size,\n",
        "    epochs=10000, \n",
        "    validation_data=data_gen_val,\n",
        "    validation_steps=len(df_val) // batch_size,\n",
        "    callbacks=[callbacks.ModelCheckpoint('sentiment.weights', save_best_only=True, save_weights_only=True)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/10000\n",
            " 539/1055 [==============>...............] - ETA: 15:00 - loss: 0.6863 - accuracy: 0.8411"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7eab674add17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_gen_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment.weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK1NMVjoJ6FE"
      },
      "source": [
        "# Rebuild model\n",
        "\n",
        "model.load_weights('model.weights')\n",
        "model.fit(, init_epochs=120, ecpohs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}